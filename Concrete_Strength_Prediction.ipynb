{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mssomie/Deep-Neural-Network/blob/main/Concrete_Strength_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdlChBp1ihRA"
      },
      "source": [
        "# Building an Artificial Neural Network from Scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iP6IgpE8A1_r"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MYnIjCqwA1l"
      },
      "source": [
        "# Define activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ne6tJwmQvJme"
      },
      "source": [
        "Logistics Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R54uOkU6iMFI"
      },
      "outputs": [],
      "source": [
        "def logisitics_function (x):\n",
        "  return 1/(1+np.exp(-x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-KiRsQEvbnT"
      },
      "source": [
        "RELU Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "__XJ4eiLvfIi"
      },
      "outputs": [],
      "source": [
        "def RELU(x):\n",
        "  return np.maximum(0,x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6qH79ZDvhBp"
      },
      "source": [
        "Hyperbolic Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lsArSz1shSKo"
      },
      "outputs": [],
      "source": [
        "def hyberbolic_tangent(x):\n",
        "    function = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "    derivative = 1 - function ** 2\n",
        "    return function, derivative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHzM-Cb9voo4",
        "outputId": "5c0a73fc-1b9f-4401-8192-a65aa3e959ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9999546021312976\n",
            "10\n",
            "(0.9999999958776926, 8.244614768671e-09)\n"
          ]
        }
      ],
      "source": [
        "print(logisitics_function(10))\n",
        "print(RELU(10))\n",
        "print(hyberbolic_tangent(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjAkg3rIv0WK"
      },
      "source": [
        "# Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hE3H5PIyhcSv"
      },
      "outputs": [],
      "source": [
        "# TODO: fix for multiple hidden layers\n",
        "class Multilayer_perceptron:\n",
        "\n",
        "   # Initialize ANN\n",
        "   def __init__(self, hidden_layers_neurons, activation_function, input_layer_neurons=8,  output_layer_neuron=1, bias_input =1):\n",
        "       self.input_layer_neurons = input_layer_neurons\n",
        "       self.hidden_layers_neurons = hidden_layers_neurons\n",
        "       self.output_layer_neuron = output_layer_neuron\n",
        "       self.weight = []\n",
        "       self.bias = []   \n",
        "\n",
        "       self.activation_function = activation_function\n",
        "\n",
        "       # Input layer to hidden layer\n",
        "       self.weight.append(np.random.rand(hidden_layers_neurons[0],input_layer_neurons))\n",
        "       self.bias.append(np.random.rand(hidden_layers_neurons[0],bias_input))\n",
        "\n",
        "      # Hidden layer to hidden layer\n",
        "       for i in range(1, len(hidden_layers_neurons)):\n",
        "        self.weight.append(np.random.rand(hidden_layers_neurons[i], hidden_layers_neurons[i-1]))\n",
        "        self.bias.append(np.random.rand(hidden_layers_neurons[i], bias_input))\n",
        "\n",
        "\n",
        "       # Hidden layer to output layer\n",
        "       self.weight.append(np.random.rand(output_layer_neuron,hidden_layers_neurons[-1]))\n",
        "       self.bias.append(np.random.rand(output_layer_neuron,bias_input))\n",
        "\n",
        "   def forward_pass(self, input):\n",
        "        # Make input a 2D array\n",
        "        input = np.array(input).reshape(-1,1)\n",
        "\n",
        "       # using Z = Wx + b\n",
        "        z_hidden=[]\n",
        "        a_hidden=[]\n",
        "        for i in range(len(self.weight)):\n",
        "            if i == 0:\n",
        "                z = np.dot(self.weight[i], input) + self.bias[i]\n",
        "            else:\n",
        "                z = np.dot(self.weight[i], a_hidden[i-1]) + self.bias[i]\n",
        "            z_hidden.append(z)\n",
        "            a_hidden.append(self.activation_function(z))\n",
        "\n",
        "        return a_hidden[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "8vsTlAATh1AU"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "shapes (8,10) and (8,1) not aligned: 10 (dim 1) != 8 (dim 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[39], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m test_multilayer_perceptron()\n",
            "Cell \u001b[0;32mIn[39], line 28\u001b[0m, in \u001b[0;36mtest_multilayer_perceptron\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Test forward pass\u001b[39;00m\n\u001b[1;32m     27\u001b[0m input_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(input_neurons, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Create random input data\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m output \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mforward_pass(input_data)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m output\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (output_neurons, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect output shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[38], line 32\u001b[0m, in \u001b[0;36mMultilayer_perceptron.forward_pass\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     30\u001b[0m z_hidden\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])):\n\u001b[0;32m---> 32\u001b[0m     z_hidden\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[i],\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias[i])\n\u001b[1;32m     33\u001b[0m     output_hidden\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_function(z_hidden[i])\n\u001b[1;32m     35\u001b[0m z_output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\u001b[38;5;28minput\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
            "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (8,10) and (8,1) not aligned: 10 (dim 1) != 8 (dim 0)"
          ]
        }
      ],
      "source": [
        "# Test the Multilayer_perceptron class\n",
        "def test_multilayer_perceptron():\n",
        "    # Define test parameters\n",
        "    input_neurons = 8\n",
        "    hidden_layers = [10, 8, 6]\n",
        "    output_neurons = 1\n",
        "    activation_func = RELU  # Using the RELU function defined earlier\n",
        "\n",
        "    # Create an instance of Multilayer_perceptron\n",
        "    mlp = Multilayer_perceptron(hidden_layers, activation_func, input_neurons, output_neurons)\n",
        "\n",
        "    # Check if the network structure is correct\n",
        "    assert len(mlp.weight) == len(hidden_layers) + 1, \"Incorrect number of weight matrices\"\n",
        "    assert len(mlp.bias) == len(hidden_layers) + 1, \"Incorrect number of bias vectors\"\n",
        "\n",
        "    # Check dimensions of weight matrices and bias vectors\n",
        "    assert mlp.weight[0].shape == (hidden_layers[0], input_neurons), \"Incorrect shape of first weight matrix\"\n",
        "    for i in range(1, len(hidden_layers)):\n",
        "        assert mlp.weight[i].shape == (hidden_layers[i], hidden_layers[i-1]), f\"Incorrect shape of weight matrix {i}\"\n",
        "    assert mlp.weight[-1].shape == (output_neurons, hidden_layers[-1]), \"Incorrect shape of output weight matrix\"\n",
        "\n",
        "    for i, neurons in enumerate(hidden_layers):\n",
        "        assert mlp.bias[i].shape == (neurons, 1), f\"Incorrect shape of bias vector {i}\"\n",
        "    assert mlp.bias[-1].shape == (output_neurons, 1), \"Incorrect shape of output bias vector\"\n",
        "\n",
        "    # Test forward pass\n",
        "    input_data = np.random.rand(input_neurons, 1)  # Create random input data\n",
        "    output = mlp.forward_pass(input_data)\n",
        "\n",
        "    assert output.shape == (output_neurons, 1), \"Incorrect output shape\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "# Run the test\n",
        "test_multilayer_perceptron()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
